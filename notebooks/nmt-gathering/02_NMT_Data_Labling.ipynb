{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM9Zc0gV0vKC"
      },
      "source": [
        "\n",
        "### Neural Machine Translation Data Labling\n",
        "\n",
        "Now that we have gathered our data and save `.csv` next we are going to create translation datasets based on the gathered tweets. We are going to create translation dataset for the following south african languages:\n",
        "\n",
        "1. Zulu\n",
        "2. Xhosa\n",
        "3. Sotho\n",
        "4. Afrikaans\n",
        "5. English\n",
        "\n",
        "___\n",
        "\n",
        "Topic: `NMT`\n",
        "\n",
        "Date: `2022/07/20`\n",
        "\n",
        "Programming Language: `python`\n",
        "\n",
        "Main: `Natural Language Processing (NLP)`\n",
        "\n",
        "___\n",
        "\n",
        "\n",
        "\n",
        "### NMT Dataset\n",
        "\n",
        "We are going to use `textblob` to create translation dataset from a huge set of english sentences which were scrapped on twitter.\n",
        "\n",
        "\n",
        "\n",
        "### Installation of `textblob`\n",
        "In the following code cell we are going to install the latest version of `textblob`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYS2rKL00vKM",
        "outputId": "5808045d-41d9-4f52-8024-79a6ba1bf355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 389 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 501 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 583 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 636 kB 6.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install textblob  --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ifGYBKPiSb"
      },
      "source": [
        "### Installing the package for helper Function\n",
        "\n",
        "We are going to install a package called [`helperfs`](https://github.com/CrispenGari/helperfns). This is the package that i've created that contains some helper function that can be useful in this task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZhJDj4doQTrE"
      },
      "outputs": [],
      "source": [
        "!pip install helperfns -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BqgT6Yn0vKW"
      },
      "source": [
        "### Importing packages\n",
        "In the following code cell we are going to import packages that we are going to use in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4An5qFaM0vKX",
        "outputId": "d1a37580-8d7d-440a-e312-973c06afe3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.17.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import textblob\n",
        "import os\n",
        "import json\n",
        "import multiprocessing\n",
        "\n",
        "from helperfns.text import clean_sentence\n",
        "\n",
        "\n",
        "textblob.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nycjxl6B0vKZ"
      },
      "source": [
        "### File System\n",
        "\n",
        "We are going to store and load data from google drive so, we need to mount the goggle drive. In the following code cell we are mounting the google drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKkslu7e2G0w",
        "outputId": "b75eda4e-e9cc-46d3-dd2f-a15f07b00896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwZ4LqyU_Ho"
      },
      "source": [
        "### Languages\n",
        "In the following code cell we are going to define our languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zlfJgr3VI-l",
        "outputId": "2073284a-d36b-486b-ba56-4a17be1a2a77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Language: <Afrikaans>,\n",
              " Language: <English>,\n",
              " Language: <Xhosa>,\n",
              " Language: <Zulu>,\n",
              " Language: <Ndebele>,\n",
              " Language: <Tsonga>,\n",
              " Language: <Tswana>,\n",
              " Language: <Swati>,\n",
              " Language: <Sotho>,\n",
              " Language: <Venda>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "class Language:\n",
        "  def __init__(self, code: str, name: str):\n",
        "    self.name = name\n",
        "    self.code = code\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return f\"Language: <{self.name}>\"\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "      return f\"Language: <{self.name}>\"\n",
        "\n",
        "languages = list(map(\n",
        "    lambda x: Language(*(x['code'], x['name'])), \n",
        "    [\n",
        "      { \"code\": \"af\", \"name\": \"Afrikaans\" },\n",
        "      { \"code\": \"en\", \"name\": \"English\" },\n",
        "      { \"code\": \"xh\", \"name\": \"Xhosa\" },\n",
        "      { \"code\": \"zu\", \"name\": \"Zulu\" },\n",
        "      { \"code\": \"nr\", \"name\": \"Ndebele\" },\n",
        "      { \"code\": \"ts\", \"name\": \"Tsonga\" },\n",
        "      { \"code\": \"tn\", \"name\": \"Tswana\" },\n",
        "      { \"code\": \"ss\", \"name\": \"Swati\" },\n",
        "      { \"code\": \"st\", \"name\": \"Sotho\" },\n",
        "      { \"code\": \"ve\", \"name\": \"Venda\" }\n",
        "    ]\n",
        "))\n",
        "\n",
        "languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEASukm7ayJE",
        "outputId": "96b92dfb-b5aa-4a22-f83e-bf0ded25d231"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Afrikaans', 'af')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "languages[0].name, languages[0].code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln9KdHsu7wj5"
      },
      "source": [
        "### Languages that are not being translated\n",
        "\n",
        "```\n",
        "ve - venda\n",
        "tn - Tswana\n",
        "ss - Swati\n",
        "nr - Ndebele (nd)\n",
        "en - English\n",
        "```\n",
        "\n",
        "The mentioned languages will be filtered out when creating the dataset for language translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09jOcBHPVyod",
        "outputId": "5e7ad9b1-f9aa-449b-9e45-674b700a99c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Language: <Afrikaans>,\n",
              " Language: <Xhosa>,\n",
              " Language: <Zulu>,\n",
              " Language: <Tsonga>,\n",
              " Language: <Sotho>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "languages = [lang for lang in languages if lang.code not in ['ve', 'tn', 'ss', 'nr', 'en']]\n",
        "languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5PCyoHEW7Fk"
      },
      "source": [
        "### Our Translation dataset\n",
        "\n",
        "Our translation dataset will contain the following south african languages to english pairs.\n",
        "\n",
        "```\n",
        "af-en\n",
        "xh-en\n",
        "zu-en\n",
        "ts-en\n",
        "st-en\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ0S_C2b2W5g"
      },
      "source": [
        "### Paths\n",
        "\n",
        "In the following code cell we are going to define the paths where all our files are stored in the google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2AbwEHzX2ghH"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/drive/My Drive/NLP Data/nmt'\n",
        "\n",
        "assert os.path.exists(base_dir), f\"The path '{base_dir}' does not exists, check if you have mounted the google drive.\"\n",
        "\n",
        "english_path = os.path.join(base_dir, \"english/english.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEmQOI4lG2x"
      },
      "source": [
        "### Reading all english sentences\n",
        "\n",
        "In the followng code cell we are going to read all english sentences and store them in a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yoOTbKaOYdRl"
      },
      "outputs": [],
      "source": [
        "all_texts = pd.read_csv(english_path).sentence.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVbbGS7Yn6Pr",
        "outputId": "79d8078c-b0ab-41e6-96f9-61f1d25edeb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108631"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(all_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8zfaEkon8Ip",
        "outputId": "d5c681e1-5e2a-41b0-ee9a-7d2e3e6ec633"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['so where are we with this eisenhower is farewell address',\n",
              "       'to you still beautiful even after death i say my farewell'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "all_texts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7c_C7MdlfPu"
      },
      "source": [
        "### `textblob_translate` function\n",
        "This function takes in a sentence and a language that we want our text to be translated and return to us a translated sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iA4n0XiPblKr"
      },
      "outputs": [],
      "source": [
        "def textblob_translate(sent: str, lang: Language) -> str:\n",
        "  blob = TextBlob(sent)\n",
        "  try:\n",
        "    return sent, str(blob.translate(from_lang='en', to=lang.code)).strip().lower()\n",
        "  except:\n",
        "    return sent, sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHrj7Z8blvBe"
      },
      "source": [
        "### Defining columns\n",
        "In the following code cell we are going to define the columns of our `csv` files. Note that there will be only two columns which are:\n",
        "\n",
        "1. `src` - the english sentences (source)\n",
        "2. `trg` - the translated version of a sentence (target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y_1jzD1JeLEJ"
      },
      "outputs": [],
      "source": [
        "columns = np.array(['src', 'trg'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va2o6iuwmL83"
      },
      "source": [
        "### `create_translation_dataset` function\n",
        "\n",
        "This function takes in a languge and does the translation and save the `csv` file in google drive.\n",
        "\n",
        "> Note that creating such a huge dataset is a huge computation and we have  `~109K` english sentences and it can take days to do this. So what we will do is to run this notebook and select a minimum number of sentences that will be translated each and every day and save them with a defined file name (where sentences pairs for translations that belongs to same language) will be in their separate folder. And then later on we will create a write code for merging these files as a single one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QYulbxrVHLGs"
      },
      "outputs": [],
      "source": [
        "start_idx = 108_000\n",
        "end_idx = 110_000 # in range [start, end] index\n",
        "day = 110\n",
        "\n",
        "def create_translation_dataset(lang: Language):\n",
        "  src_trg = list()\n",
        "  folder_path = os.path.join(base_dir, 'nmt_datasets', f'en-{lang.code}')\n",
        "  if not os.path.exists(folder_path):\n",
        "    os.mkdir(folder_path)\n",
        "  save_path = os.path.join(folder_path, f'en-{lang.code}-{day}.csv')\n",
        "  if lang.code != 'en':\n",
        "    print()\n",
        "    print(f\"Creating translation dataset for en-{lang.code}\")\n",
        "    for txt in all_texts[start_idx:end_idx]:\n",
        "      try:\n",
        "        src_trg.append(textblob_translate(txt, lang))\n",
        "      except (textblob.exceptions.NotTranslated, TypeError, Exception):\n",
        "        continue\n",
        "    print(f\"Done creating translation dataset for en-{lang.code}\")\n",
        "  if len(src_trg) > 0:\n",
        "    print(f\"Got {len(src_trg)} examples for en-{lang.code}\")\n",
        "    dataframe = pd.DataFrame(src_trg, columns=columns, index=None)\n",
        "    dataframe.to_csv(save_path)\n",
        "    print(f\"Saved en-{lang.code} dataset as en-{lang.code}-{day}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JpzSbVryPNZ"
      },
      "source": [
        "### Creating the dataset for each language.\n",
        "\n",
        "In the next code cell we are going to create the dataset for each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwm3Ffh3-rko",
        "outputId": "434a1964-58f0-4f07-9205-eb4b72643f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating translation dataset for en-af\n",
            "Done creating translation dataset for en-af\n",
            "Got 630 examples for en-af\n",
            "Saved en-af dataset as en-af-110.csv\n",
            "\n",
            "Creating translation dataset for en-xh\n",
            "Done creating translation dataset for en-xh\n",
            "Got 630 examples for en-xh\n",
            "Saved en-xh dataset as en-xh-110.csv\n",
            "\n",
            "Creating translation dataset for en-zu\n",
            "Done creating translation dataset for en-zu\n",
            "Got 630 examples for en-zu\n",
            "Saved en-zu dataset as en-zu-110.csv\n",
            "\n",
            "Creating translation dataset for en-ts\n",
            "Done creating translation dataset for en-ts\n",
            "Got 630 examples for en-ts\n",
            "Saved en-ts dataset as en-ts-110.csv\n",
            "\n",
            "Creating translation dataset for en-st\n",
            "Done creating translation dataset for en-st\n",
            "Got 630 examples for en-st\n",
            "Saved en-st dataset as en-st-110.csv\n"
          ]
        }
      ],
      "source": [
        "for lang in languages:\n",
        "  create_translation_dataset(lang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJM2wAEEXZkO"
      },
      "source": [
        "### References\n",
        "\n",
        "1. [List_of_ISO_639-1_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "02_NMT_Data_Labling.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}